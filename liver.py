# -*- coding: utf-8 -*-
"""liverviva.ipynb
each comment line is a new cell in colab


Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYfC_z6hHtd6YC6Ko1YBXs7QZLl-pPOy
"""

import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
import seaborn as sns
import matplotlib.pyplot as plt

dataset = pd.read_csv("/content/indian_liver_patient.csv")
dataset.head()

dataset.describe()

dataset['Gender'].replace(['Female','Male'],[0,1],inplace=True)

dataset=dataset.drop_duplicates()

dataset.duplicated().sum()

dataset.isna().sum()



from sklearn.impute import IterativeImputer


# Setting the random_state argument for reproducibility
imputer = IterativeImputer(random_state=42)
imputed = imputer.fit_transform(dataset)
df_imputed = pd.DataFrame(imputed, columns=dataset.columns)

round(df_imputed, 2)
dataset=df_imputed

Dataset_data = dataset['Dataset']
gender_data = dataset['Gender']
dropped_columns = [ 'Gender', 'Dataset']
dataset = dataset.drop(dropped_columns, axis=1)
dataset

Age_data = dataset['Age']

Totalprotiens_data = dataset['Total_Protiens']
Albumin_data = dataset['Albumin']
dropped_columns = ['Age','Total_Protiens','Albumin']
dataset = dataset.drop(dropped_columns, axis=1)
dataset

# Load the required libraries
from scipy.stats import skew
# Check the skewness of the data
skewness = dataset.apply(lambda x: skew(x.dropna()))
print(skewness)

# Transform the skewed data using the log transformation
transformed_data = dataset.copy()
for column in dataset.columns:
    if pd.api.types.is_numeric_dtype(dataset[column]):
        if skew(dataset[column].dropna()) > 0.5:
            transformed_data[column] = np.log1p(dataset[column].dropna())

# Check the transformed data
print(transformed_data)
dataset=transformed_data


# Visualize the transformed data with density plots
for column in transformed_data.columns:
    if pd.api.types.is_numeric_dtype(transformed_data[column]):
        plt.figure(figsize=(8, 6))
        plt.title(f'Distribution of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
        sns.histplot(transformed_data[column], kde=True, color='skyblue', bins=30)
        # Add density plot
        sns.kdeplot(transformed_data[column], color='red', linestyle='dashed', label='Density')

        plt.legend()
        plt.show()

dataset.isna().sum()

dataset['Gender'] = gender_data
dataset['Dataset'] = Dataset_data
dataset['Age'] = Age_data
dataset['Total_Protiens'] = Totalprotiens_data
dataset['Albumin'] = Albumin_data

dataset

# Load the required libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'Dataset' is the name of your target variable column
# Create separate datasets for samples with and without disease
with_disease = dataset[dataset['Dataset'] == 1]
without_disease = dataset[dataset['Dataset'] == 2]

# Define the width of the bars
bar_width = 0.3  # Adjust this value according to your preference

# Plot histograms for each dataset
plt.figure(figsize=(8, 6))

# Plot with disease
sns.histplot(with_disease['Dataset'], color='orangered', label='With Disease', bins=2, discrete=True, alpha=0.7, stat='count', kde=False, linewidth=0, element='bars', common_norm=False, common_bins=False, shrink=0.8, binwidth=bar_width)

# Plot without disease with a gap
sns.histplot(without_disease['Dataset'], color='teal', label='Without Disease', bins=2, discrete=True, alpha=0.7, stat='count', kde=False, linewidth=0, element='bars', common_norm=False, common_bins=False, shrink=0.8, binwidth=bar_width)

# Adjust x-axis limits to create a small gap between bars
plt.xlim(0.5, 2.5)

plt.xlabel('Dataset')
plt.ylabel('Count')
plt.title('Distribution of Target Variable with and without Disease')
plt.legend()
plt.show()

import pandas as pd
from sklearn.utils import resample

# Load the dataset
# Separate majority and minority classes
df_majority = dataset[dataset['Dataset'] == 1]  # Records with liver disease
df_minority = dataset[dataset['Dataset'] == 2]  # Records without liver disease

# Upsample minority class to match the number of records in the majority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,     # Sample with replacement
                                 n_samples=len(df_majority),    # Match the number of records in the majority class
                                 random_state=42)  # Set random state for reproducibility

# Combine majority class with upsampled minority class
df_balanced = pd.concat([df_majority, df_minority_upsampled])

# Shuffle the dataframe
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Now df_balanced contains a balanced dataset with equal number of records for each class

dataset=df_balanced
dataset.shape
dataset.isna().sum()

dataset.duplicated().sum()

# Load the required libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'Dataset' is the name of your target variable column
# Create separate datasets for samples with and without disease
with_disease = dataset[dataset['Dataset'] == 1]
without_disease = dataset[dataset['Dataset'] == 2]

# Define the width of the bars
bar_width = 0.3  # Adjust this value according to your preference

# Plot histograms for each dataset
plt.figure(figsize=(8, 6))

# Plot with disease
sns.histplot(with_disease['Dataset'], color='orangered', label='With Disease', bins=2, discrete=True, alpha=0.7, stat='count', kde=False, linewidth=0, element='bars', common_norm=False, common_bins=False, shrink=0.8, binwidth=bar_width)

# Plot without disease with a gap
sns.histplot(without_disease['Dataset'], color='teal', label='Without Disease', bins=2, discrete=True, alpha=0.7, stat='count', kde=False, linewidth=0, element='bars', common_norm=False, common_bins=False, shrink=0.8, binwidth=bar_width)

# Adjust x-axis limits to create a small gap between bars
plt.xlim(0.5, 2.5)

plt.xlabel('Dataset')
plt.ylabel('Count')
plt.title('Distribution of Target Variable with and without Disease')
plt.legend()
plt.show()

Dataset_data = dataset['Dataset']
gender_data = dataset['Gender']
dropped_columns = [ 'Gender', 'Dataset']
dataset = dataset.drop(dropped_columns, axis=1)
dataset

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()

# Fit the scaler to the data and transform it
scaled_data = scaler.fit_transform(dataset)

# Convert the scaled data back to a DataFrame (optional)
scaled_dfr = pd.DataFrame(scaled_data, columns=dataset.columns)

# Display the scaled DataFrame
print(scaled_dfr.head())


# Load liver dataset


# Visual Inspection: Box Plot
sns.boxplot(data=scaled_dfr)
plt.title('Box Plot')
plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility
plt.show()

scaled_dfr.shape

scaled_dfr['Gender'] = gender_data
scaled_dfr['Dataset'] = Dataset_data
scaled_dfr

import scipy.stats as stats
from scipy.stats import chi2_contingency, f_oneway
from sklearn.feature_selection import mutual_info_classif, f_classif

# Assuming the dataset is stored in a pandas DataFrame called 'df'
# and the target variable is stored in a pandas Series called 'target'

# Perform chi-square test for gender feature
chi2_stat, p_val, dof, ex = stats.chi2_contingency(pd.crosstab(scaled_dfr.Gender,scaled_dfr.Dataset))
print(f"Chi-square statistic: {chi2_stat}, p-value: {p_val}")

# Perform f-test for numerical features
f_stat, p_val = f_classif(scaled_dfr.drop('Gender', axis=1),scaled_dfr.Dataset)
for i, j in enumerate(f_stat):
    print(f"F-statistic for {scaled_dfr.columns[i]}: {j}, p-value: {p_val[i]}")
# Perform f-test for numerical features
f_scores = f_oneway(*[scaled_dfr[feature] for feature in scaled_dfr.select_dtypes(include=['float64', 'int64']).columns])

# Perform mutual information test for all features
mi_scores = mutual_info_classif(scaled_dfr.drop(['Gender', 'Dataset'], axis=1), scaled_dfr['Dataset'])

# Perform mutual information test for feature selection
mi = mutual_info_classif(scaled_dfr.drop('Gender', axis=1), scaled_dfr.Dataset)
mi_df = pd.Series(mi, index=scaled_dfr.columns[:-1])
mi_df.sort_values(ascending=False).plot.bar(figsize=(10, 5))
plt.ylabel('Mutual Information')
plt.title("Mutual information between predictors and target")
plt.show()

chi2_threshold = 0.05
f_threshold = 0.05
mi_threshold = 0.1
features_to_keep = []
for feature, p_value in enumerate(f_scores):
    if p_value > f_threshold:
        features_to_keep.append(scaled_dfr.columns[feature+2])
for feature, score in enumerate(mi_scores):
    if score > mi_threshold:
        features_to_keep.append(scaled_dfr.columns[feature+2])

# Create the final dataset by keeping only the selected features
final_data = scaled_dfr[features_to_keep + ['Dataset']]




import seaborn as sns

# Calculate the correlation matrix
correlation_matrix = final_data.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(8,6))
sns.heatmap(correlation_matrix, annot=True, cmap='magma', fmt=".2f", annot_kws={"size": 10})
plt.title('Correlation Matrix')
plt.show()
final_data

features=['Alkaline_Phosphotase','Aspartate_Aminotransferase','Alamine_Aminotransferase','Albumin_and_Globulin_Ratio', 'Age', 'Dataset']
df=scaled_dfr[features].copy()
df

from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn import utils
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import roc_auc_score
X2=df.drop('Dataset', axis=1)
y2_transformed=df['Dataset']

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y2= lab.fit_transform(y2_transformed)

#view transformed values
print(y2_transformed)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, random_state=42)

rf=RandomForestClassifier(n_estimators=100, criterion='gini', min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, random_state=42)
rf.fit(X2_train,y2_train)
# Make predictions on the testing set
y1_pred=rf.predict(X2_test)

# Calculate the accuracy of the classifier
accuracy1 = accuracy_score(y2_test, y1_pred)
print("Accuracy of new model:", accuracy1)

conf_matrix = confusion_matrix(y2_test, y1_pred)

print(conf_matrix)
precision1 = precision_score(y2_test, y1_pred)

print("Precision Score:", precision1)

# Calculate recall
recall1 = recall_score(y2_test, y1_pred)

print("Recall Score:", recall1 )

# Calculate F1-score
f1r = f1_score(y2_test, y1_pred)
print("F1 Score:", f1r)
roc_auc1 = roc_auc_score(y2_test, y1_pred)
print("ROC AUC score:", roc_auc1)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(rf, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores1=scores.mean()
print("Mean cross validation score:",scores1 )

plt.figure(figsize=(4,4))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

pip install umap-learn

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
# Create and train ExtraTreesClassifier
extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=42)



# Predictions
y_pred = extra_trees.predict(X2_test)

# Calculate accuracy
accuracy2 = accuracy_score(y2_test, y_pred)
print("Accuracy:", accuracy2)
conf_matrix = confusion_matrix(y2_test, y_pred)

print(conf_matrix)
precision2 = precision_score(y2_test, y_pred)
print("Precision Score:", precision2)

# Calculate recall
recall2 = recall_score(y2_test, y_pred)
print("Recall Score:", recall2)
roc_auc2 = roc_auc_score(y2_test, y_pred)
print("ROC AUC score:", roc_auc2)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(extra_trees, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores2=scores.mean()
print("Mean cross validation score:",scores2 )
# Calculate F1-score
f1e = f1_score(y2_test, y_pred)
print("F1 Score:", f1e)
plt.figure(figsize=(2,2))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()
importances = extra_trees.feature_importances_
labels = ['blue' if label == 0 else 'red' for label in y2]

# Create a t-SNE plot
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

# Reduce dimensionality using t-SNE
# Create a UMAP plot
umap_plot = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')
X_umap = umap_plot.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
# Create base classifier (Decision Tree)
base_classifier = DecisionTreeClassifier()

# Create BaggingClassifier
bagging_classifier = BaggingClassifier( n_estimators=100, random_state=42)

# Train BaggingClassifier
bagging_classifier.fit(X2_train, y2_train)

# Predictions
y_pred = bagging_classifier.predict(X2_test)

# Calculate accuracy
accuracy3 = accuracy_score(y2_test, y_pred)
print("Accuracy:", accuracy3)


conf_matrix = confusion_matrix(y2_test, y_pred)

print(conf_matrix)
precision3 = precision_score(y2_test, y_pred)
print("Precision Score:", precision3)

# Calculate recall
recall3 = recall_score(y2_test, y_pred)
print("Recall Score:", recall3)

# Calculate F1-score
f1b = f1_score(y2_test, y_pred)
print("F1 Score:", f1b)
roc_auc3 = roc_auc_score(y2_test, y_pred)
print("ROC AUC score:", roc_auc3)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(bagging_classifier, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores3=scores.mean()
print("Mean cross validation score:",scores3 )
plt.figure(figsize=(2,2))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=2)

# Fit the classifier to the training data
clf.fit(X2_train, y2_train)

# Calculate the accuracy of the classifier on the test data
accuracy4 = clf.score(X2_test, y2_test)
yG_pred = clf.predict(X2_test)
print("Accuracy :",accuracy4)
conf=confusion_matrix(y2_test, yG_pred)
print(conf)
precision4 = precision_score(y2_test,yG_pred)

print("Precision Score:",precision4 )

# Calculate recall
recall4 = recall_score(y2_test,yG_pred)

print("Recall Score:", recall4)

# Calculate F1-score
f1g = f1_score(y2_test,yG_pred)
print("F1 Score:", f1g)
roc_auc4 = roc_auc_score(y2_test, yG_pred)
print("ROC AUC score:", roc_auc4)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(clf, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores4=scores.mean()
print("Mean cross validation score:",scores4 )
plt.figure(figsize=(2,2))
sns.heatmap(conf, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

import xgboost as xgb
from sklearn.metrics import accuracy_score
# Initialize and train XGBoost classifier
xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1,  gamma=1, reg_alpha=1, reg_lambda=0.5, base_score=0.5, booster='gbtree', random_state=42)
xgb_classifier.fit(X2_train, y2_train)
# Predict classes
Yx_pred = xgb_classifier.predict(X2_test)

# Evaluate the model
accuracy5 = accuracy_score(y2_test, Yx_pred)
print("Accuracy:", accuracy5)

precision5 = precision_score(y2_test, Yx_pred)
print("Precision Score:", precision5)

# Calculate recall
recall5 = recall_score(y2_test, Yx_pred)
print("Recall Score:", recall5)

# Calculate F1-score
f1x = f1_score(y2_test, Yx_pred)
print("F1 Score:", f1x)
roc_auc5 = roc_auc_score(y2_test, Yx_pred)
print("ROC AUC score:", roc_auc5)
conf1=confusion_matrix(y2_test, Yx_pred)
print(conf1)
from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(xgb_classifier, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores5=scores.mean()
print("Mean cross validation score:",scores5 )
plt.figure(figsize=(2,2))
sns.heatmap(conf1, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error, mean_squared_error

import numpy as np

# Step 1: Define base learners
base_learners = [
    ExtraTreesClassifier(),
    RandomForestClassifier(),
    XGBClassifier() # Adding XGBoost
]

# Step 2: Define meta learner
meta_learner = LogisticRegression()

# Step 3: Train base learners
base_learners_predictions_train = []
for learner in base_learners:
    learner.fit(X2_train, y2_train)
    base_learners_predictions_train.append(learner.predict(X2_train))

# Step 4: Train meta learner
meta_learner_input_train = np.column_stack(base_learners_predictions_train)
meta_learner.fit(meta_learner_input_train, y2_train)

# Step 4: Testing
base_learners_predictions_test = []
for learner in base_learners:
    base_learners_predictions_test.append(learner.predict(X2_test))

meta_learner_input_test = np.column_stack(base_learners_predictions_test)
final_predictions = meta_learner.predict(meta_learner_input_test)

# Evaluate final predictions
accuracy6 = accuracy_score(y2_test, final_predictions)
print("Accuracy:", accuracy6)

conf_matrix = confusion_matrix(y2_test,final_predictions )

print(conf_matrix)
precision6 = precision_score(y2_test, final_predictions)
print("Precision Score:",precision6 )

# Calculate recall
recall6 = recall_score(y2_test, final_predictions)
print("Recall Score:", recall6 )

# Calculate F1-score
f1s = f1_score(y2_test, final_predictions)
print("F1 Score:", f1s)
roc_auc6 = roc_auc_score(y2_test, final_predictions)
print("ROC AUC score:", roc_auc6)
mae = mean_absolute_error(y2_test, final_predictions)
print("Mean Absolute Error (MAE):", mae)

# Calculate MSE
mse = mean_squared_error(y2_test, final_predictions)
print("Mean Squared Error (MSE):", mse)

# Calculate RMSE
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)


from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(learner, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores6=scores.mean()
print("Mean cross validation score:",scores6 )

plt.figure(figsize=(4,4))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()
labels = ['blue' if label == 0 else 'red' for label in y2]

# Create a t-SNE plot
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

# Reduce dimensionality using t-SNE
# Create a UMAP plot
umap_plot = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')
X_umap = umap_plot.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

import numpy as np
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.manifold import TSNE
import umap
# Define the base learners
base_learners = [
    ('SVM', SVC(random_state=42)),
    ('KNN', KNeighborsClassifier(n_neighbors=5)),
    ('Extra Tree', ExtraTreesClassifier(n_estimators=100,random_state=42)),
    ('Random Forest', RandomForestClassifier(n_estimators=100,random_state=42)),
    ('XGBoost', XGBClassifier(random_state=42)),
    ('Bagging', BaggingClassifier(n_estimators=100,random_state=42)),
    ('GradientBoosting', GradientBoostingClassifier(random_state=42)),
    ('DecisionTree', DecisionTreeClassifier())
]

# Define the meta learner
meta_learner = LogisticRegression(random_state=42)

# Create the stacking classifier
stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5
)

# Train the stacking classifier
stacking_clf.fit(X2_train, y2_train)

# Make predictions on the testing set
ys_pred = stacking_clf.predict(X2_test)

# Evaluate the performance of the stacking classifier
accuracy7 = accuracy_score(y2_test, ys_pred)
print("Accuracy:", accuracy7)

conf_matrix = confusion_matrix(y2_test,ys_pred )

print(conf_matrix)
precision7 = precision_score(y2_test, ys_pred)
print("Precision Score:",precision7 )

# Calculate recall
recall7 = recall_score(y2_test, ys_pred)
print("Recall Score:", recall7 )

# Calculate F1-score
f1m = f1_score(y2_test, ys_pred)
print("F1 Score:", f1m)
roc_auc7 = roc_auc_score(y2_test, ys_pred)
print("ROC AUC score:", roc_auc7)
mae = mean_absolute_error(y2_test, ys_pred)
print("Mean Absolute Error (MAE):", mae)

# Calculate MSE
mse = mean_squared_error(y2_test, ys_pred)
print("Mean Squared Error (MSE):", mse)

# Calculate RMSE
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(stacking_clf, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores7=scores.mean()
print("Mean cross validation score:",scores7 )


plt.figure(figsize=(4,4))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

labels = ['blue' if label == 0 else 'red' for label in y2]

# Create a t-SNE plot
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

# Reduce dimensionality using t-SNE
# Create a UMAP plot
umap_plot = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')
X_umap = umap_plot.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()


def predict_liver_disease(Alkaline_Phosphotase, Aspartate_Aminotransferase, Alamine_Aminotransferase, Albumin_and_Globulin_Ratio, Age):
    input_data = np.array([[Alkaline_Phosphotase, Aspartate_Aminotransferase, Alamine_Aminotransferase, Albumin_and_Globulin_Ratio, Age]])
    prediction = stacking_clf.predict(input_data)
    if prediction[0] == 2:
        return "The person is predicted to not have liver disease"
    else:
        return "The person is predicted to have liver disease."

# Example usage
Alkaline_Phosphotase = 100
Aspartate_Aminotransferase = 300
Alamine_Aminotransferase = 260
Albumin_and_Globulin_Ratio = 160
Age = 45

prediction = predict_liver_disease(Alkaline_Phosphotase, Aspartate_Aminotransferase, Alamine_Aminotransferase, Albumin_and_Globulin_Ratio, Age)
print("Prediction:", prediction)

print("| Model             | Accuracy                      | Precision                           | Recall                     |")
print("|-----------------  |-------------------------------|-------------------------------------|----------------------------|")
print(f"| Random Forest     | {accuracy1}            | {precision1}                  | {recall1}          |")
print(f"| Extra Tree Forest | {accuracy2}            | {precision2}                  | {recall2}          |")
print(f"| Bagging           | {accuracy3}            | {precision3}                  | {recall3}          |")
print(f"| Gradient boosting | {accuracy4}            | {precision4}                         | {recall4}         |")
print(f"| xgboosting        | {accuracy5}            | {precision5}                  | {recall5}          |")
print(f"| stacking          | {accuracy6}            | {precision6}                  | {recall6}          |")
print(f"| new model         | {accuracy7}             | {precision7}                                | {recall7}          |")

import matplotlib.pyplot as plt

# Define model names
models = ['Random Forest', 'Extra Tree Forest', 'Bagging', 'Gradient Boosting', 'XGBoost', 'Stacking']

# Define metric values for each model
accuracy = [accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6]
precision = [precision1, precision2, precision3, precision4, precision5, precision6]
recall = [recall1, recall2, recall3, recall4, recall5, recall6]

# Define the width of each bar
bar_width = 0.3

# Set up the positions for the bars
index = range(len(models))

# Plot each metric
plt.figure(figsize=(12, 8))

plt.bar(index, accuracy, bar_width, label='Accuracy')
plt.bar([i + bar_width for i in index], precision, bar_width, label='Precision')
plt.bar([i + 2 * bar_width for i in index], recall, bar_width, label='Recall')

# Add labels and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('Score', fontsize=14)
plt.title('Metrics Comparison between Different Models', fontsize=16)
plt.xticks([i + bar_width for i in index], models, fontsize=12, rotation=45)
plt.yticks(fontsize=12)
plt.legend(fontsize=12)

# Show plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define model names
models = ['Random Forest', 'Extra Tree Forest', 'Bagging', 'Gradient Boosting', 'XGBoost', 'Stacking']

# Define ROC AUC scores for each model
roc_auc_scores = [roc_auc1, roc_auc2, roc_auc3, roc_auc4, roc_auc5, roc_auc6]

# Set up the positions for the points on the x-axis
x_pos = range(1, len(models) + 1)

# Plot ROC AUC scores for each model
plt.figure(figsize=(10, 6))
plt.plot(x_pos, roc_auc_scores, marker='o', linestyle='-')

# Add labels and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('ROC AUC Score', fontsize=14)
plt.title('ROC AUC Score Comparison between Different Models', fontsize=16)
plt.xticks(x_pos, models, fontsize=12, rotation=45)
plt.grid()

# Show plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define model names
models = ['Random Forest', 'Extra Tree Forest', 'Bagging', 'Gradient Boosting', 'XGBoost', 'Stacking']
score13=0.93
# Define ROC AUC scores for each model
score = [scores1, scores2, scores3, scores4, scores5, score13]

# Set up the positions for the points on the x-axis
x_pos = range(1, len(models) + 1)

# Plot ROC AUC scores for each model
plt.figure(figsize=(10, 6))
plt.plot(x_pos, score, marker='o', linestyle='-')

# Add labels and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('10fold cross validation', fontsize=14)
plt.title('10 cross validation Comparison between Different Models', fontsize=16)
plt.xticks(x_pos, models, fontsize=12, rotation=45)
plt.grid()

# Show plot
plt.tight_layout()
plt.show()
