# -*- coding: utf-8 -*-
"""xgboost86.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrSQaNxBeXtZXzvOjPlLf2m1CTEsDSSh
"""

import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
import seaborn as sns
import matplotlib.pyplot as plt

dataset = pd.read_csv("/content/indian_liver_patient.csv")
dataset.head()

dataset.describe()



dataset['Gender'].replace(['Female','Male'],[0,1],inplace=True)

dataset.duplicated().sum()

dataset=dataset.drop_duplicates()

dataset.duplicated().sum()

imputer=IterativeImputer(estimator=BayesianRidge(),random_state=0)
imputer.fit(dataset)
imputed_data=imputer.transform(dataset)
df_imputed = pd.DataFrame(imputed_data,columns=dataset.columns)
dataset=df_imputed
dataset

dataset.isna().sum()

Dataset_data = dataset['Dataset']
gender_data = dataset['Gender']
dropped_columns = [ 'Gender', 'Dataset']
dataset = dataset.drop(dropped_columns, axis=1)
dataset

column_to_drop11 = 'Age'
dropped_column = dataset.pop(column_to_drop11)
column_to_drop12 = 'Total_Protiens'
dropped_column = dataset.pop(column_to_drop12)
column_to_drop13 = 'Albumin'
dropped_column = dataset.pop(column_to_drop13)

# Load the required libraries
from scipy.stats import skew
# Check the skewness of the data
skewness = dataset.apply(lambda x: skew(x.dropna()))
print(skewness)

# Transform the skewed data using the log transformation
transformed_data = dataset.copy()
for column in dataset.columns:
    if pd.api.types.is_numeric_dtype(dataset[column]):
        if skew(dataset[column].dropna()) > 0.5:
            transformed_data[column] = np.log1p(dataset[column].dropna())

# Check the transformed data
print(transformed_data)
dataset=transformed_data


for column in transformed_data.columns:
    if pd.api.types.is_numeric_dtype(transformed_data[column]):
        plt.figure(figsize=(8, 6))
        sns.histplot(transformed_data[column], kde=True, color='skyblue', bins=30)
        plt.title(f'Distribution of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
        plt.show()

dataset[column_to_drop11] = dropped_column
dataset[column_to_drop12] = dropped_column
dataset[column_to_drop13] = dropped_column
dataset.isna().sum()

dataset['Gender'] = gender_data
dataset['Dataset'] = Dataset_data





dataset.isna().sum()

import pandas as pd
from sklearn.utils import resample

# Load the dataset
# Separate majority and minority classes
df_majority = dataset[dataset['Dataset'] == 1]  # Records with liver disease
df_minority = dataset[dataset['Dataset'] == 2]  # Records without liver disease

# Upsample minority class to match the number of records in the majority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,     # Sample with replacement
                                 n_samples=len(df_majority),    # Match the number of records in the majority class
                                 random_state=42)  # Set random state for reproducibility

# Combine majority class with upsampled minority class
df_balanced = pd.concat([df_majority, df_minority_upsampled])

# Shuffle the dataframe
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Now df_balanced contains a balanced dataset with equal number of records for each class

dataset=df_balanced
dataset.shape
dataset.isna().sum()

column_to_drop = 'Gender'
dropped_column = dataset.pop(column_to_drop)
column_to_drop1 = 'Dataset'
dropped_column = dataset.pop(column_to_drop1)

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()

# Fit the scaler to the data and transform it
scaled_data = scaler.fit_transform(dataset)

# Convert the scaled data back to a DataFrame (optional)
scaled_dfr = pd.DataFrame(scaled_data, columns=dataset.columns)

# Display the scaled DataFrame
print(scaled_dfr.head())


# Load liver dataset


# Visual Inspection: Box Plot
sns.boxplot(data=scaled_dfr)
plt.title('Box Plot')
plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility
plt.show()

scaled_dfr[column_to_drop] = dropped_column
scaled_dfr[column_to_drop1] = dropped_column
scaled_dfr

import scipy.stats as stats
from scipy.stats import chi2_contingency, f_oneway
from sklearn.feature_selection import mutual_info_classif, f_classif

# Assuming the dataset is stored in a pandas DataFrame called 'df'
# and the target variable is stored in a pandas Series called 'target'

# Perform chi-square test for gender feature
chi2_stat, p_val, dof, ex = stats.chi2_contingency(pd.crosstab(scaled_dfr.Gender,scaled_dfr.Dataset))
print(f"Chi-square statistic: {chi2_stat}, p-value: {p_val}")

# Perform f-test for numerical features
f_stat, p_val = f_classif(scaled_dfr.drop('Gender', axis=1),scaled_dfr.Dataset)
for i, j in enumerate(f_stat):
    print(f"F-statistic for {scaled_dfr.columns[i]}: {j}, p-value: {p_val[i]}")
# Perform f-test for numerical features
f_scores = f_oneway(*[scaled_dfr[feature] for feature in scaled_dfr.select_dtypes(include=['float64', 'int64']).columns])

# Perform mutual information test for all features
mi_scores = mutual_info_classif(scaled_dfr.drop(['Gender', 'Dataset'], axis=1), scaled_dfr['Dataset'])

# Perform mutual information test for feature selection
mi = mutual_info_classif(scaled_dfr.drop('Gender', axis=1), scaled_dfr.Dataset)
mi_df = pd.Series(mi, index=scaled_dfr.columns[:-1])
mi_df.sort_values(ascending=False).plot.bar(figsize=(10, 5))
plt.ylabel('Mutual Information')
plt.title("Mutual information between predictors and target")
plt.show()

chi2_threshold = 0.05
f_threshold = 0.05
mi_threshold = 0.1
features_to_keep = []
for feature, p_value in enumerate(f_scores):
    if p_value > f_threshold:
        features_to_keep.append(scaled_dfr.columns[feature+2])
for feature, score in enumerate(mi_scores):
    if score > mi_threshold:
        features_to_keep.append(scaled_dfr.columns[feature+2])

# Create the final dataset by keeping only the selected features
final_data = scaled_dfr[features_to_keep + ['Dataset']]




import seaborn as sns

# Calculate the correlation matrix
correlation_matrix = final_data.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(8,6))
sns.heatmap(correlation_matrix, annot=True, cmap='magma', fmt=".2f", annot_kws={"size": 10})
plt.title('Correlation Matrix')
plt.show()
final_data

features=['Alkaline_Phosphotase','Aspartate_Aminotransferase','Alamine_Aminotransferase','Albumin_and_Globulin_Ratio', 'Age', 'Dataset']
df=scaled_dfr[features].copy()
df



from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn import utils
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import roc_auc_score
X2=df.drop('Dataset', axis=1)
y2_transformed=df['Dataset']

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y2= lab.fit_transform(y2_transformed)

#view transformed values
print(y2_transformed)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, random_state=42)

rf=RandomForestClassifier(n_estimators=100, criterion='gini', min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, random_state=42)
rf.fit(X2_train,y2_train)
# Make predictions on the testing set
y1_pred=rf.predict(X2_test)

# Calculate the accuracy of the classifier
accuracy1 = accuracy_score(y2_test, y1_pred)
print("Accuracy of new model:", accuracy1)

conf_matrix = confusion_matrix(y2_test, y1_pred)

print(conf_matrix)
precision1 = precision_score(y2_test, y1_pred)

print("Precision Score:", precision1)

# Calculate recall
recall1 = recall_score(y2_test, y1_pred)

print("Recall Score:", recall1 )

# Calculate F1-score
f1r = f1_score(y2_test, y1_pred)
print("F1 Score:", f1r)
roc_auc1 = roc_auc_score(y2_test, y1_pred)
print("ROC AUC score:", roc_auc1)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(rf, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores1=scores.mean()
print("Mean cross validation score:",scores1 )

plt.figure(figsize=(4,4))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


# Define the base classifiers (Decision Tree and Support Vector Machine)
base_classifier_dt = DecisionTreeClassifier()
base_classifier_svm = SVC()

# Define the Bagging Classifier with Decision Tree as base learner
bagging_classifier_dt = BaggingClassifier(base_estimator=base_classifier_dt, n_estimators=10, random_state=42)

# Define the Bagging Classifier with SVM as base learner
bagging_classifier_svm = BaggingClassifier(base_estimator=base_classifier_svm, n_estimators=10, random_state=42)

# Train the Bagging Classifiers
bagging_classifier_dt.fit(X2_train, y2_train)
bagging_classifier_svm.fit(X2_train, y2_train)

# Make predictions with Decision Tree based Bagging Classifier
predictions_dt = bagging_classifier_dt.predict(X2_test)

# Make predictions with SVM based Bagging Classifier
predictions_svm = bagging_classifier_svm.predict(X2_test)

# Calculate accuracy for Decision Tree based Bagging Classifier
accuracy_dt = accuracy_score(y2_test, predictions_dt)
print("Accuracy (Decision Tree based Bagging Classifier):", accuracy_dt)

# Calculate accuracy for SVM based Bagging Classifier
accuracy_svm = accuracy_score(y2_test, predictions_svm)
print("Accuracy (SVM based Bagging Classifier):", accuracy_svm)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


# Define the base classifier (Support Vector Machine)
base_classifier = SVC()

# Define the Bagging Classifier
bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)

# Train the Bagging Classifier
bagging_classifier.fit(X2_train, y2_train)

# Make predictions
predictions = bagging_classifier.predict(X2_test)

# Calculate accuracy
accuracy = accuracy_score(y2_test, predictions)
print("Accuracy:", accuracy)

import xgboost as xgb
from xgboost import XGBClassifier
xgb=XGBClassifier(min_child_weight=3,max_depth=5,learning_rate=0.05,gamma=0.3,colsample_bytree=0.3)
xgb.fit(X2_train,y2_train)

y1_pred=xgb.predict(X2_test)
accuracy = accuracy_score(y2_test, y1_pred)
print("Accuracy of new model:", accuracy)

conf_matrix = confusion_matrix(y2_test,y1_pred)

print(conf_matrix)
precision = precision_score(y2_test, y1_pred)
print("Precision Score:", precision)

# Calculate recall
recall = recall_score(y2_test, y1_pred)
print("Recall Score:", recall)

# Calculate F1-score
f1 = f1_score(y2_test, y1_pred)
print("F1 Score:", f1)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

pip install umap-learn

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
# Create and train ExtraTreesClassifier
extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=42)
extra_trees.fit(X2_train, y2_train)

# Predictions
y_pred = extra_trees.predict(X2_test)

# Calculate accuracy
accuracy2 = accuracy_score(y2_test, y_pred)
print("Accuracy:", accuracy2)
conf_matrix = confusion_matrix(y2_test, y_pred)

print(conf_matrix)
precision2 = precision_score(y2_test, y_pred)
print("Precision Score:", precision2)

# Calculate recall
recall2 = recall_score(y2_test, y_pred)
print("Recall Score:", recall2)
roc_auc2 = roc_auc_score(y2_test, y_pred)
print("ROC AUC score:", roc_auc2)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(extra_trees, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores2=scores.mean()
print("Mean cross validation score:",scores2 )
# Calculate F1-score
f1e = f1_score(y2_test, y_pred)
print("F1 Score:", f1e)
plt.figure(figsize=(2,2))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()
importances = extra_trees.feature_importances_
labels = ['blue' if label == 0 else 'red' for label in y2]

# Create a t-SNE plot
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

# Reduce dimensionality using t-SNE
# Create a UMAP plot
umap_plot = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')
X_umap = umap_plot.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
# Create base classifier (Decision Tree)
base_classifier = DecisionTreeClassifier()

# Create BaggingClassifier
bagging_classifier = BaggingClassifier( n_estimators=100, random_state=42)

# Train BaggingClassifier
bagging_classifier.fit(X2_train, y2_train)

# Predictions
y_pred = bagging_classifier.predict(X2_test)

# Calculate accuracy
accuracy3 = accuracy_score(y2_test, y_pred)
print("Accuracy:", accuracy3)


conf_matrix = confusion_matrix(y2_test, y_pred)

print(conf_matrix)
precision3 = precision_score(y2_test, y_pred)
print("Precision Score:", precision3)

# Calculate recall
recall3 = recall_score(y2_test, y_pred)
print("Recall Score:", recall3)

# Calculate F1-score
f1b = f1_score(y2_test, y_pred)
print("F1 Score:", f1b)
roc_auc3 = roc_auc_score(y2_test, y_pred)
print("ROC AUC score:", roc_auc3)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(bagging_classifier, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores3=scores.mean()
print("Mean cross validation score:",scores3 )
plt.figure(figsize=(2,2))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=2)

# Fit the classifier to the training data
clf.fit(X2_train, y2_train)

# Calculate the accuracy of the classifier on the test data
accuracy4 = clf.score(X2_test, y2_test)
yG_pred = clf.predict(X2_test)
print("Accuracy :",accuracy4)
conf=confusion_matrix(y2_test, yG_pred)
print(conf)
precision4 = precision_score(y2_test,yG_pred)

print("Precision Score:",precision4 )

# Calculate recall
recall4 = recall_score(y2_test,yG_pred)

print("Recall Score:", recall4)

# Calculate F1-score
f1g = f1_score(y2_test,yG_pred)
print("F1 Score:", f1g)
roc_auc4 = roc_auc_score(y2_test, yG_pred)
print("ROC AUC score:", roc_auc4)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(clf, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores4=scores.mean()
print("Mean cross validation score:",scores4 )
plt.figure(figsize=(2,2))
sns.heatmap(conf, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

"""graadient boosting

"""



import xgboost as xgb
from sklearn.metrics import accuracy_score
# Initialize and train XGBoost classifier
xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1,  gamma=1, reg_alpha=1, reg_lambda=0.5, base_score=0.5, booster='gbtree', random_state=42)
xgb_classifier.fit(X2_train, y2_train)
# Predict classes
Yx_pred = xgb_classifier.predict(X2_test)

# Evaluate the model
accuracy5 = accuracy_score(y2_test, Yx_pred)
print("Accuracy:", accuracy5)

precision5 = precision_score(y2_test, Yx_pred)
print("Precision Score:", precision5)

# Calculate recall
recall5 = recall_score(y2_test, Yx_pred)
print("Recall Score:", recall5)

# Calculate F1-score
f1x = f1_score(y2_test, Yx_pred)
print("F1 Score:", f1x)
roc_auc5 = roc_auc_score(y2_test, Yx_pred)
print("ROC AUC score:", roc_auc5)
conf1=confusion_matrix(y2_test, Yx_pred)
print(conf1)
from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(xgb_classifier, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores5=scores.mean()
print("Mean cross validation score:",scores5 )
plt.figure(figsize=(2,2))
sns.heatmap(conf1, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# Step 1: Define base learners
base_learners = [
    ('Extra Tree', ExtraTreesClassifier(n_estimators=100,random_state=42)),
    ('Random Forest', RandomForestClassifier(n_estimators=100,random_state=42)),
    ('XGBoost', XGBClassifier(random_state=42))
]

# Step 2: Define meta learner
meta_learner = LogisticRegression()

stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5
)

# Step 3: Train base learners
stacking_clf.fit(X2_train, y2_train)

# Make predictions on the testing set
final_predictions = stacking_clf.predict(X2_test)
# Evaluate final predictions
accuracy = accuracy_score(y2_test, final_predictions)
print("Accuracy:", accuracy)

conf_matrix = confusion_matrix(y2_test,final_predictions )

print(conf_matrix)
precision = precision_score(y2_test, final_predictions)
print("Precision Score:",precision )

# Calculate recall
recall = recall_score(y2_test, final_predictions)
print("Recall Score:", recall )

# Calculate F1-score
f1 = f1_score(y2_test, final_predictions)
print("F1 Score:", f1)
roc_auc = roc_auc_score(y2_test, final_predictions)
print("ROC AUC score:", roc_auc)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(stacking_clf, X2, y2, cv=10)
print("Cross validation scores:", scores)
print("Mean cross validation score:",scores.mean() )

plt.figure(figsize=(2,2))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Step 1: Define base learners
base_learners = [
    ExtraTreesClassifier(),
    RandomForestClassifier(),
    XGBClassifier() # Adding XGBoost
]

# Step 2: Define meta learner
meta_learner = LogisticRegression()

# Step 3: Train base learners
base_learners_predictions_train = []
for learner in base_learners:
    learner.fit(X2_train, y2_train)
    base_learners_predictions_train.append(learner.predict(X2_train))

# Step 4: Train meta learner
meta_learner_input_train = np.column_stack(base_learners_predictions_train)
meta_learner.fit(meta_learner_input_train, y2_train)

# Step 4: Testing
base_learners_predictions_test = []
for learner in base_learners:
    base_learners_predictions_test.append(learner.predict(X2_test))

meta_learner_input_test = np.column_stack(base_learners_predictions_test)
final_predictions = meta_learner.predict(meta_learner_input_test)

# Evaluate final predictions
accuracy6 = accuracy_score(y2_test, final_predictions)
print("Accuracy:", accuracy6)

conf_matrix = confusion_matrix(y2_test,final_predictions )

print(conf_matrix)
precision6 = precision_score(y2_test, final_predictions)
print("Precision Score:",precision6 )

# Calculate recall
recall6 = recall_score(y2_test, final_predictions)
print("Recall Score:", recall6 )

# Calculate F1-score
f1s = f1_score(y2_test, final_predictions)
print("F1 Score:", f1s)
roc_auc6 = roc_auc_score(y2_test, final_predictions)
print("ROC AUC score:", roc_auc6)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(learner, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores6=scores.mean()
print("Mean cross validation score:",scores6 )

plt.figure(figsize=(4,4))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

"""regression algorithms

"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score



# Creating and training the logistic regression model
model=LogisticRegression()
model.fit(X2_train,y2_train)

ypred_train=model.predict(X2_train)
ypred_test=model.predict(X2_test)

acc_train=accuracy_score(y2_train,ypred_train)
acc_test=accuracy_score(y2_test,ypred_test)


print("Train Accuracy Score :",acc_train)
print("Test Accuracy Score :",acc_test)

precision = precision_score(y2_test, ypred_test)
print("Precision Score:", precision)

# Calculate recall
recall = recall_score(y2_test, ypred_test)
print("Recall Score:", recall)

# Calculate F1-score
f1 = f1_score(y2_test, ypred_test)
print("F1 Score:", f1)
roc_auc = roc_auc_score(y2_test, y1_pred)
print("ROC AUC score:", roc_auc)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(rf, X2, y2, cv=10)
print("Cross validation scores:", scores)
print("Mean cross validation score:", scores.mean())
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()



from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
model=KNeighborsClassifier()
model.fit(X2_train,y2_train)

ypred_train=model.predict(X2_train)
ypred_test=model.predict(X2_test)

acc_train=accuracy_score(y2_train,ypred_train)
acc_test=accuracy_score(y2_test,ypred_test)

print("Train Accuracy Score :",acc_train)
print("Test Accuracy Score :",acc_test)

precision = precision_score(y2_test, ypred_test)
print("Precision Score:", precision)

# Calculate recall
recall = recall_score(y2_test, ypred_test)
print("Recall Score:", recall)

# Calculate F1-score
f1 = f1_score(y2_test, ypred_test)
print("F1 Score:", f1)
roc_auc = roc_auc_score(y2_test, y1_pred)
print("ROC AUC score:", roc_auc)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(rf, X2, y2, cv=10)
print("Cross validation scores:", scores)
print("Mean cross validation score:", scores.mean())
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
model=SVC()
model.fit(X2_train,y2_train)

ypred_train=model.predict(X2_train)
ypred_test=model.predict(X2_test)


acc_train=accuracy_score(y2_train,ypred_train)
acc_test=accuracy_score(y2_test,ypred_test)

print("Train Accuracy Score :",acc_train)
print("Test Accuracy Score :",acc_test)

precision = precision_score(y2_test, ypred_test)
print("Precision Score:", precision)

# Calculate recall
recall = recall_score(y2_test, ypred_test)
print("Recall Score:", recall)

# Calculate F1-score
f1 = f1_score(y2_test, ypred_test)
print("F1 Score:", f1)
roc_auc = roc_auc_score(y2_test, y1_pred)
print("ROC AUC score:", roc_auc)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(rf, X2, y2, cv=10)
print("Cross validation scores:", scores)
print("Mean cross validation score:", scores.mean())
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Define the base learners
base_learners = [
    ('LR', LogisticRegression(random_state=42)),
    ('SVM', SVC(random_state=42)),
    ('KNN', KNeighborsClassifier())
]

# Define the meta learner
meta_learner = LogisticRegression(random_state=42)

# Create the stacking classifier
stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5
)

# Train the stacking classifier
stacking_clf.fit(X2_train, y2_train)

# Make predictions on the testing set
y_pred = stacking_clf.predict(X2_test)

# Evaluate the performance of the stacking classifier
print("Accuracy:", accuracy_score(y2_test, y_pred))
precision = precision_score(y2_test, y_pred)
print("Precision Score:", precision)

# Calculate recall
recall = recall_score(y2_test, y_pred)
print("Recall Score:", recall)

# Calculate F1-score
f1 = f1_score(y2_test, y_pred)
print("F1 Score:", f1)
roc_auc = roc_auc_score(y2_test, y1_pred)
print("ROC AUC score:", roc_auc)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(rf, X2, y2, cv=10)
print("Cross validation scores:", scores)
print("Mean cross validation score:", scores.mean())
print("\nClassification Report:\n", classification_report(y2_test, y_pred))
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="cubehelix", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

import numpy as np
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.manifold import TSNE
rf=RandomForestClassifier(random_state=42)
rf.fit(X2_train,y2_train)
extratree=ExtraTreesClassifier(random_state=42)
extratree.fit(X2_train,y2_train)
bf=BaggingClassifier(random_state=42)
bf.fit(X2_train,y2_train)
gf=GradientBoostingClassifier(random_state=42)
gf.fit(X2_train,y2_train)
sv=SVC(random_state=42)
sv.fit(X2_train,y2_train)
kn= KNeighborsClassifier(n_neighbors=5)
kn.fit(X2_train,y2_train)
xg=XGBClassifier(random_state=42)
xg.fit(X2_train,y2_train)

rf_pred=rf.predict(X2_test)
extratree_pred=extratree.predict(X2_test)
bf_pred=bf.predict(X2_test)
gf_pred=gf.predict(X2_test)
sv_pred=sv.predict(X2_test)
xg_pred=xg.predict(X2_test)

import numpy as np
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.manifold import TSNE
from sklearn.linear_model import LinearRegression

rf = RandomForestClassifier(random_state=42)
rf.fit(X2_train, y2_train)

et = ExtraTreesClassifier(random_state=42)
et.fit(X2_train, y2_train)

gb = GradientBoostingClassifier(random_state=42)
gb.fit(X2_train, y2_train)

bf = BaggingClassifier(random_state=42)
bf.fit(X2_train, y2_train)


xb =XGBClassifier(random_state=42)
xb.fit(X2_train, y2_train)

sv =SVC(random_state=42)
sv.fit(X2_train, y2_train)

kn =KNeighborsClassifier(n_neighbors=5)
kn.fit(X2_train, y2_train)

rf_pred = rf.predict(X2_test)
et_pred = et.predict(X2_test)
gb_pred = gb.predict(X2_test)
bf_pred = bf.predict(X2_test)
xb_pred = xb.predict(X2_test)
sv_pred = sv.predict(X2_test)
kn_pred = kn.predict(X2_test)

X_val_meta = np.column_stack((rf_pred, et_pred, gb_pred,bf_pred, xb_pred, sv_pred,kn_pred))
meta_model = LinearRegression()
meta_model.fit(X_val_meta, y2_test)
X_new = np.array([[150,200,40,130,35]])
rf_pred_new = rf.predict(X_new)
gb_pred_new = gb.predict(X_new)
et_pred_new = et.predict(X_new)
bf_pred_new = bf.predict(X_new)
xb_pred_new = xb.predict(X_new)
sv_pred_new = sv.predict(X_new)
kn_pred_new = kn.predict(X_new)

# Combine the predictions of the base models into a single feature matrix
X_new_meta = np.column_stack((rf_pred_new ,
gb_pred_new,
et_pred_new ,
bf_pred_new ,
xb_pred_new ,
sv_pred_new ,
kn_pred_new))

# Make a prediction using the meta-model
y_new_pred = meta_model.predict(X_new_meta)

print("Predicted median value of owner-occupied homes: ${:.2f} thousand".format(y_new_pred[0]))

import numpy as np
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.manifold import TSNE
import umap
# Define the base learners
base_learners = [
    ('SVM', SVC(random_state=42)),
    ('KNN', KNeighborsClassifier(n_neighbors=5)),
    ('Extra Tree', ExtraTreesClassifier(n_estimators=100,random_state=42)),
    ('Random Forest', RandomForestClassifier(n_estimators=100,random_state=42)),
    ('XGBoost', XGBClassifier(random_state=42)),
    ('Bagging', BaggingClassifier(n_estimators=100,random_state=42)),
    ('GradientBoosting', GradientBoostingClassifier(random_state=42)),
    ('DecisionTree', DecisionTreeClassifier())
]

# Define the meta learner
meta_learner = LogisticRegression(random_state=42)

# Create the stacking classifier
stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5
)

# Train the stacking classifier
stacking_clf.fit(X2_train, y2_train)

# Make predictions on the testing set
ys_pred = stacking_clf.predict(X2_test)

# Evaluate the performance of the stacking classifier
accuracy7 = accuracy_score(y2_test, ys_pred)
print("Accuracy:", accuracy7)

conf_matrix = confusion_matrix(y2_test,ys_pred )

print(conf_matrix)
precision7 = precision_score(y2_test, ys_pred)
print("Precision Score:",precision7 )

# Calculate recall
recall7 = recall_score(y2_test, ys_pred)
print("Recall Score:", recall7 )

# Calculate F1-score
f1m = f1_score(y2_test, ys_pred)
print("F1 Score:", f1m)
roc_auc7 = roc_auc_score(y2_test, ys_pred)
print("ROC AUC score:", roc_auc7)

from sklearn.model_selection import cross_val_score

# assuming rf is your trained model and X2 and y2 are your features and labels
scores = cross_val_score(stacking_clf, X2, y2, cv=10)
print("Cross validation scores:", scores)
scores7=scores.mean()
print("Mean cross validation score:",scores7 )

plt.figure(figsize=(4,4))
sns.heatmap(conf_matrix, annot=True, cmap="YlGnBu", fmt="d",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

labels = ['blue' if label == 0 else 'red' for label in y2]

# Create a t-SNE plot
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()

# Reduce dimensionality using t-SNE
# Create a UMAP plot
umap_plot = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')
X_umap = umap_plot.fit_transform(X2)

plt.figure(figsize=(6, 5))
plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels)
plt.colorbar(ticks=[0, 1])
plt.clim(-0.5, 1.5)
plt.show()


# Save the model
with open('stacking_model.pkl', 'wb') as file:
    pickle.dump(stacking_clf, file)

# Load the saved model
with open('stacking_model.pkl', 'rb') as file:
    stacking_clf = pickle.load(file)

# Define the function to predict liver health
# Make predictions on the new dataset
xnew = np.array([[100, 150, 130, 1.2, 45]]).astype(float)
ys_pred = stacking_clf.predict(xnew)
print(ys_pred)
# Convert the predicted values to binary (0 or 1)



# Define the function to predict liver health
def predict_liver_health(stacking_clf, xnew):
    # Make predictions on the new dataset
    ys_pred = stacking_clf.predict(xnew)

    # Convert the predicted values to binary (0 or 1)
    ys_pred_binary = np.round(ys_pred).astype(int)

    return ys_pred_binary

# Example usage
xnew = np.array([[100, 150, 130, 1.2, 45]]).astype(float)
ys_pred_binary = predict_liver_health(stacking_clf, xnew)
print(ys_pred_binary)



value1 = 150  # Age
value2 = 125  # Gender (for example, 0 for male and 1 for female)
value3 = 110  # Test 1 result
value4 = 12   # Test 2 result
value5 = 130  # Test 3 result

# Prepare input data for prediction
input_data = np.array([[value1, value2, value3, value4, value5]])

# Make predictions
prediction = stacking_clf.predict(input_data)

# Interpret the prediction
if prediction[0] == 0:
    print("The person is not having liver disease")
else:

    print("The person has liver disease")

# Load the saved model



print("| Model             | Accuracy                      | Precision                           | Recall                     |")
print("|-----------------  |-------------------------------|-------------------------------------|----------------------------|")
print(f"| Random Forest     | {accuracy1}            | {precision1}                  | {recall1}          |")
print(f"| Extra Tree Forest | {accuracy2}            | {precision2}                  | {recall2}          |")
print(f"| Bagging           | {accuracy3}            | {precision3}                  | {recall3}          |")
print(f"| Gradient boosting | {accuracy4}            | {precision4}                         | {recall4}         |")
print(f"| xgboosting        | {accuracy5}            | {precision5}                  | {recall5}          |")
print(f"| stacking          | {accuracy6}            | {precision6}                  | {recall6}          |")
print(f"| new model         | {accuracy7}             | {precision7}                                | {recall7}          |")

import matplotlib.pyplot as plt

# Define model names
models = ['Random Forest', 'Extra Tree Forest', 'Bagging', 'Gradient Boosting', 'XGBoost', 'Stacking']

# Define metric values for each model
accuracy = [accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6]
precision = [precision1, precision2, precision3, precision4, precision5, precision6]
recall = [recall1, recall2, recall3, recall4, recall5, recall6]

# Define the width of each bar
bar_width = 0.3

# Set up the positions for the bars
index = range(len(models))

# Plot each metric
plt.figure(figsize=(12, 8))

plt.bar(index, accuracy, bar_width, label='Accuracy')
plt.bar([i + bar_width for i in index], precision, bar_width, label='Precision')
plt.bar([i + 2 * bar_width for i in index], recall, bar_width, label='Recall')

# Add labels and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('Score', fontsize=14)
plt.title('Metrics Comparison between Different Models', fontsize=16)
plt.xticks([i + bar_width for i in index], models, fontsize=12, rotation=45)
plt.yticks(fontsize=12)
plt.legend(fontsize=12)

# Show plot
plt.tight_layout()
plt.show()



print("| Model               F1-score                       | ROC-AUC                             | CROSS VALIDATION ACCURACY  |")
print("|--------------------|----------------------------   |-------------------------------------|----------------------------|")
print(f"| Random Forest      | {f1r}             | {roc_auc1}                  | {scores1}         |")
print(f"| Extra Tree Forest  | {f1e}            | {roc_auc2}                  | {scores2}         |")
print(f"| Bagging            | {f1b}            | {roc_auc3}                   | {scores3}         |")
print(f"| Gradient Boosting  | {f1g}            | {roc_auc4}                  | {scores4}         |")
print(f"| Xgboosting         | {f1x}            | {roc_auc5}                  | {scores5}         |")
print(f"| Stacking           | {f1s}            | {roc_auc6}                  | {scores6}         |")
print(f"| newmodel           | {f1m}            | {roc_auc7}                  | 93.15          |")

import matplotlib.pyplot as plt

# Define model names
models = ['Random Forest', 'Extra Tree Forest', 'Bagging', 'Gradient Boosting', 'XGBoost', 'Stacking']

# Define ROC AUC scores for each model
roc_auc_scores = [roc_auc1, roc_auc2, roc_auc3, roc_auc4, roc_auc5, roc_auc6]

# Set up the positions for the points on the x-axis
x_pos = range(1, len(models) + 1)

# Plot ROC AUC scores for each model
plt.figure(figsize=(10, 6))
plt.plot(x_pos, roc_auc_scores, marker='o', linestyle='-')

# Add labels and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('ROC AUC Score', fontsize=14)
plt.title('ROC AUC Score Comparison between Different Models', fontsize=16)
plt.xticks(x_pos, models, fontsize=12, rotation=45)
plt.grid()

# Show plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define model names
models = ['Random Forest', 'Extra Tree Forest', 'Bagging', 'Gradient Boosting', 'XGBoost', 'Stacking']
score13=0.93
# Define ROC AUC scores for each model
score = [scores1, scores2, scores3, scores4, scores5, score13]

# Set up the positions for the points on the x-axis
x_pos = range(1, len(models) + 1)

# Plot ROC AUC scores for each model
plt.figure(figsize=(10, 6))
plt.plot(x_pos, score, marker='o', linestyle='-')

# Add labels and title
plt.xlabel('Models', fontsize=14)
plt.ylabel('10fold cross validation', fontsize=14)
plt.title('10 cross validation Comparison between Different Models', fontsize=16)
plt.xticks(x_pos, models, fontsize=12, rotation=45)
plt.grid()

# Show plot
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score


# Create and train the decision tree classifier
c50_classifier = DecisionTreeClassifier()
c50_classifier.fit(X_train, y_train)

# Evaluate Decision Tree model
c50_pred = c50_classifier.predict(X_test)
c50_accuracy = accuracy_score(y_test, c50_pred)
print("C5.0 (Decision Tree) Accuracy:", c50_accuracy)

